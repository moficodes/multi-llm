{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d9bbf86da5e"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99c1c3fc2ca5"
      },
      "source": [
        "# Gemma deployment to GKE using vLLM on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQXDY7p6utC"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/moficodes/multi-llm/blob/main/notebooks/serve_gemma_on_gke_using_vllm.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3de7470326a2"
      },
      "source": [
        "## Overview\n",
        "\n",
        "This notebook demonstrates downloading and deploying Gemma, open models from Google DeepMind using vLLM, an efficient serving option to improve serving throughput. In this notebook we will deploy and serve TGI on GPUs. In this guide we specifically use L4 GPUs but this guide should also work for A100(40 GB), A100(80 GB), H100(80 GB) GPUs.\n",
        "\n",
        "\n",
        "### Objective\n",
        "\n",
        "Deploy and run inference for serving Gemma with vLLM on GPUs.\n",
        "\n",
        "### GPUs\n",
        "\n",
        "GPUs let you accelerate specific workloads running on your nodes such as machine learning and data processing. GKE provides a range of machine type options for node configuration, including machine types with NVIDIA H100, L4, and A100 GPUs.\n",
        "\n",
        "Before you use GPUs in GKE, we recommend that you complete the following learning path:\n",
        "\n",
        "Learn about [current GPU version availability](https://cloud.google.com/compute/docs/gpus)\n",
        "\n",
        "Learn about [GPUs in GKE](https://cloud.google.com/kubernetes-engine/docs/concepts/gpus)\n",
        "\n",
        "\n",
        "### vLLM\n",
        "\n",
        "vLLM is a fast and easy-to-use library for LLM inference and serving.\n",
        "\n",
        "Originally developed in the Sky Computing Lab at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "264c07757582"
      },
      "source": [
        "## Before you begin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35Dvbzb0hH3-"
      },
      "source": [
        "### Configure Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c460088b873"
      },
      "source": [
        "Set the following variables for the experiment environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "855d6b96f291"
      },
      "outputs": [],
      "source": [
        "# The HuggingFace token used to download models.\n",
        "HF_TOKEN = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "# The size of the model to launch\n",
        "MODEL_SIZE = \"1b\"  # @param [\"1b\", \"4b\", \"12b\"]\n",
        "# Cloud project id.\n",
        "PROJECT_ID = \"mofilabs\"  # @param {type:\"string\"}\n",
        "# Region for launching clusters.\n",
        "REGION = \"us-central1\"  # @param {type:\"string\"}\n",
        "# The cluster name to create\n",
        "CLUSTER_NAME = \"gke-llm-inference\"  # @param {type:\"string\"}\n",
        "\n",
        "# The number of GPUs to run: 1 for 1b or 4b, 2 for 12b\n",
        "GPU_COUNT = 1\n",
        "if MODEL_SIZE == \"12b\":\n",
        "    GPU_COUNT = 2\n",
        "# Ephemeral storage\n",
        "EPHEMERAL_STORAGE_SIZE = \"10Gi\"\n",
        "if MODEL_SIZE == \"4b\":\n",
        "    EPHEMERAL_STORAGE_SIZE = \"20Gi\"\n",
        "if MODEL_SIZE == \"12b\":\n",
        "    EPHEMERAL_STORAGE_SIZE = \"40Gi\"\n",
        "\n",
        "# Memory size\n",
        "MEMORY_SIZE = \"10Gi\"\n",
        "if MODEL_SIZE == \"4b\":\n",
        "    MEMORY_SIZE = \"20Gi\"\n",
        "if MODEL_SIZE == \"12b\":\n",
        "    MEMORY_SIZE = \"40Gi\"\n",
        "GPU_SHARD = 1\n",
        "if MODEL_SIZE == \"12b\":\n",
        "    GPU_SHARD = 2\n",
        "CPU_LIMITS = 4\n",
        "if MODEL_SIZE == \"12b\":\n",
        "    CPU_LIMITS = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klPAnx16cVd7"
      },
      "outputs": [],
      "source": [
        "! gcloud config set project \"$PROJECT_ID\"\n",
        "! gcloud services enable container.googleapis.com\n",
        "\n",
        "# Add kubectl to the set of available tools.\n",
        "! mkdir -p /tools/google-cloud-sdk/.install\n",
        "! gcloud components install kubectl --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e828eb320337"
      },
      "source": [
        "### Create a GKE cluster and a node pool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKhdKv1vK9Lg"
      },
      "source": [
        "GKE creates the following resources for the model based on the MODEL_SIZE environment variable set above.\n",
        "\n",
        "- Autopilot cluster\n",
        "\n",
        "If you already have a cluster, you can skip to `Use an existing GKE cluster` instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cd25839741"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters create-auto {CLUSTER_NAME} \\\n",
        "  --project={PROJECT_ID} \\\n",
        "  --region={REGION} \\\n",
        "  --release-channel=rapid \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ydvYk7FLJz_"
      },
      "source": [
        "### Use an existing GKE cluster"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmpNpYF-LRut",
        "outputId": "b0a0fd68-ff9e-42a8-8a9a-3c0cc3a86615"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fetching cluster endpoint and auth data.\n",
            "kubeconfig entry generated for gke-llm-inference.\n"
          ]
        }
      ],
      "source": [
        "! gcloud container clusters get-credentials {CLUSTER_NAME} --location {REGION}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cc825514deb"
      },
      "source": [
        "### Create Kubernetes secret for Hugging Face credentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgZfNOSyOY7_"
      },
      "source": [
        "Create a Kubernetes Secret that contains the Hugging Face token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b42bd4fa2b2d",
        "outputId": "8d4519ff-4eda-49d3-f9ae-fc8bcb51063c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "secret/hf-secret created\n"
          ]
        }
      ],
      "source": [
        "! kubectl create secret generic hf-secret \\\n",
        "--from-literal=hf_api_token={HF_TOKEN} \\\n",
        "--dry-run=client -o yaml | kubectl apply -f -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c250872074f"
      },
      "source": [
        "### Deploy vLLM\n",
        "\n",
        "Use the YAML to deploy Gemma on vLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6psJZY_zUDgj",
        "outputId": "4be2a38b-0b73-43f4-c6a3-79b524ebd4fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "apiVersion: apps/v1\n",
            "kind: Deployment\n",
            "metadata:\n",
            "  name: vllm-gemma-deployment\n",
            "spec:\n",
            "  replicas: 1\n",
            "  selector:\n",
            "    matchLabels:\n",
            "      app: vllm-gemma-server\n",
            "  template:\n",
            "    metadata:\n",
            "      labels:\n",
            "        app: vllm-gemma-server\n",
            "        ai.gke.io/model: gemma-3-1b-it\n",
            "        ai.gke.io/inference-server: vllm\n",
            "        examples.ai.gke.io/source: user-guide\n",
            "    spec:\n",
            "      containers:\n",
            "      - name: inference-server\n",
            "        image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250312_0916_RC01\n",
            "        resources:\n",
            "          requests:\n",
            "            cpu: 4\n",
            "            memory: 10Gi\n",
            "            ephemeral-storage: 10Gi\n",
            "            nvidia.com/gpu: 1\n",
            "          limits:\n",
            "            cpu: 4\n",
            "            memory: 10Gi\n",
            "            ephemeral-storage: 10Gi\n",
            "            nvidia.com/gpu: 1\n",
            "        command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n",
            "        args:\n",
            "        - --model=$(MODEL_ID)\n",
            "        - --tensor-parallel-size=1\n",
            "        - --host=0.0.0.0\n",
            "        - --port=8000\n",
            "        env:\n",
            "        - name: MODEL_ID\n",
            "          value: google/gemma-3-1b-it\n",
            "        - name: HUGGING_FACE_HUB_TOKEN\n",
            "          valueFrom:\n",
            "            secretKeyRef:\n",
            "              name: hf-secret\n",
            "              key: hf_api_token\n",
            "        volumeMounts:\n",
            "        - mountPath: /dev/shm\n",
            "          name: dshm\n",
            "      volumes:\n",
            "      - name: dshm\n",
            "        emptyDir:\n",
            "            medium: Memory\n",
            "      nodeSelector:\n",
            "        cloud.google.com/gke-accelerator: nvidia-l4\n",
            "        cloud.google.com/gke-gpu-driver-version: latest\n",
            "---\n",
            "apiVersion: v1\n",
            "kind: Service\n",
            "metadata:\n",
            "  name: vllm-llm-service\n",
            "spec:\n",
            "  selector:\n",
            "    app: vllm-gemma-server\n",
            "  type: ClusterIP\n",
            "  ports:\n",
            "    - protocol: TCP\n",
            "      port: 8000\n",
            "      targetPort: 8000\n"
          ]
        }
      ],
      "source": [
        "K8S_YAML = f\"\"\"\n",
        "apiVersion: apps/v1\n",
        "kind: Deployment\n",
        "metadata:\n",
        "  name: vllm-gemma-deployment\n",
        "spec:\n",
        "  replicas: 1\n",
        "  selector:\n",
        "    matchLabels:\n",
        "      app: vllm-gemma-server\n",
        "  template:\n",
        "    metadata:\n",
        "      labels:\n",
        "        app: vllm-gemma-server\n",
        "        ai.gke.io/model: gemma-3-{MODEL_SIZE}-it\n",
        "        ai.gke.io/inference-server: vllm\n",
        "        examples.ai.gke.io/source: user-guide\n",
        "    spec:\n",
        "      containers:\n",
        "      - name: inference-server\n",
        "        image: us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20250312_0916_RC01\n",
        "        resources:\n",
        "          requests:\n",
        "            cpu: {CPU_LIMITS}\n",
        "            memory: {MEMORY_SIZE}\n",
        "            ephemeral-storage: {EPHEMERAL_STORAGE_SIZE}\n",
        "            nvidia.com/gpu: {GPU_COUNT}\n",
        "          limits:\n",
        "            cpu: {CPU_LIMITS}\n",
        "            memory: {MEMORY_SIZE}\n",
        "            ephemeral-storage: {EPHEMERAL_STORAGE_SIZE}\n",
        "            nvidia.com/gpu: {GPU_COUNT}\n",
        "        command: [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n",
        "        args:\n",
        "        - --model=$(MODEL_ID)\n",
        "        - --tensor-parallel-size={GPU_SHARD}\n",
        "        - --host=0.0.0.0\n",
        "        - --port=8000\n",
        "        env:\n",
        "        - name: MODEL_ID\n",
        "          value: google/gemma-3-{MODEL_SIZE}-it\n",
        "        - name: HUGGING_FACE_HUB_TOKEN\n",
        "          valueFrom:\n",
        "            secretKeyRef:\n",
        "              name: hf-secret\n",
        "              key: hf_api_token\n",
        "        volumeMounts:\n",
        "        - mountPath: /dev/shm\n",
        "          name: dshm\n",
        "      volumes:\n",
        "      - name: dshm\n",
        "        emptyDir:\n",
        "            medium: Memory\n",
        "      nodeSelector:\n",
        "        cloud.google.com/gke-accelerator: nvidia-l4\n",
        "        cloud.google.com/gke-gpu-driver-version: latest\n",
        "---\n",
        "apiVersion: v1\n",
        "kind: Service\n",
        "metadata:\n",
        "  name: vllm-llm-service\n",
        "spec:\n",
        "  selector:\n",
        "    app: vllm-gemma-server\n",
        "  type: ClusterIP\n",
        "  ports:\n",
        "    - protocol: TCP\n",
        "      port: 8000\n",
        "      targetPort: 8000\n",
        "\"\"\"\n",
        "\n",
        "with open(\"vllm.yaml\", \"w\") as f:\n",
        "    f.write(K8S_YAML)\n",
        "\n",
        "! cat vllm.yaml\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkKxoMwHMnbW",
        "outputId": "7d1bb725-512f-4b29-c283-37c9b7fcc1d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "deployment.apps/vllm-gemma-deployment created\n",
            "service/vllm-llm-service created\n"
          ]
        }
      ],
      "source": [
        "! kubectl apply -f vllm.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYMesXi7WqCu"
      },
      "source": [
        "#### Waiting for the container to create"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKwbzKXuWvoL"
      },
      "source": [
        "Use the command below to check on the status of the container."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PXbPCrWtWqbk"
      },
      "outputs": [],
      "source": [
        "! kubectl get pod -w"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzINwFr_WVAB"
      },
      "source": [
        "#### View the logs from the running deployment\n",
        "\n",
        "##### This will download the needed artifacts, this process will take close to 5 minutes depending on what runtime you are using to run your colab environment. The server is up and running and ready to take inference request once you see log messages like these :\n",
        "\n",
        "```\n",
        "INFO:     Started server process [1]\n",
        "INFO:     Waiting for application startup.\n",
        "INFO:     Application startup complete.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPKVtosbWYjg"
      },
      "outputs": [],
      "source": [
        "! kubectl logs -f -l app=vllm-gemma-server"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7Q_oG4TW6sD"
      },
      "source": [
        "#### Set up port forwarding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDTasPgGW7EG",
        "outputId": "5e228e8c-3288-43d7-8568-450068c6aae1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"id\":\"chatcmpl-2aa7c4dc097442c391b0cd2dc5e66d6c\",\"object\":\"chat.completion\",\"created\":1742696545,\"model\":\"google/gemma-3-4b-it\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"reasoning_content\":null,\"content\":\"Okay, let's break down why the sky is blue! It's a surprisingly complex phenomenon, but here's the simplified explanation:\\n\\n**1. Sunlight and Colors:**\\n\\n* Sunlight is actually made up of all the colors of the rainbow – red, orange, yellow, green, blue, indigo, and violet.  Think of a prism splitting sunlight into those colors.\\n\\n**2. The Role of the Atmosphere (specifically, Rayleigh Scattering):**\\n\\n* **Air Molecules:** Earth’s atmosphere isn't empty space. It's filled with tiny air molecules – mostly nitrogen and oxygen.\\n* **Scattering:** When sunlight enters the atmosphere, it collides with these air molecules. This causes the light to *scatter* in different directions.\\n* **Rayleigh Scattering:**  Here's the key: **Rayleigh scattering** is the phenomenon where shorter wavelengths of light (blue and violet) are scattered *much more* effectively than longer wavelengths (red and orange). \\n\\n**3. Why Blue, Not Violet?**\\n\\n* **Violet is Scattered More:** Violet light is actually scattered even *more* than blue light. However, there are a couple of reasons why we see blue instead:\\n    * **Sun's Spectrum:** The sun emits slightly less violet light than blue light.\\n    * **Earth's Atmosphere:** The atmosphere absorbs some violet light.\\n    * **Our Eyes:** Our eyes are more sensitive to blue light than violet light.\\n\\n\\n**In short, blue light is scattered all over the sky because it's bounced around by the air molecules.**\\n\\n\\n**Think of it like this:** Imagine throwing a small ball (blue light) and a large ball (red light) at a bunch of small obstacles in a room. The small ball will bounce around more easily than the large ball.\\n\\n\\n**Bonus:** \\n\\n* **Sunsets & Sunrises:** At sunset and sunrise, the sunlight has to travel through much more of the atmosphere to reach your eyes.  This means most of the blue light is scattered away, and we're left with the longer wavelengths – primarily red and orange – which are more visible.\\n\\n---\\n\\n**Resources for Further Learning:**\\n\\n* **NASA - Atmospheric Scattering:** [https://science.nasa.gov/space-exploration/earth-orbiting-systems/atmosphere/scattering/](https://science.nasa.gov/space-exploration/earth-orbiting-systems/atmosphere/scattering/)\\n* **Science Candy - Why is the sky blue?** [https://www.sciencecandy.com/why-is-the-sky-blue/](https://www.sciencecandy.com/why-is-the-sky-blue/)\\n\\n\\nWould you like me to elaborate on any of these points, or perhaps discuss a different phenomenon related to the sky?\",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":106}],\"usage\":{\"prompt_tokens\":15,\"total_tokens\":603,\"completion_tokens\":588,\"prompt_tokens_details\":null},\"prompt_logprobs\":null}"
          ]
        }
      ],
      "source": [
        "! kubectl exec -t $( kubectl get pod -l app=vllm-gemma-server -o jsonpath=\"{.items[0].metadata.name}\" ) \\\n",
        "   -c inference-server -- curl -X POST http://localhost:8000/v1/chat/completions \\\n",
        "    -X POST \\\n",
        "    -H \"Content-Type: application/json\" \\\n",
        "    -d '{\"model\": \"google/gemma-3-4b-it\",\"messages\": [{\"role\": \"user\", \"content\": \"Why is the sky blue?\"}]}' \\\n",
        "    2> /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af21a3cff1e0"
      },
      "source": [
        "## Clean up resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911406c1561e"
      },
      "outputs": [],
      "source": [
        "! kubectl delete deployments vllm-gemma-deployment\n",
        "! kubectl delete services vllm-llm-service\n",
        "! kubectl delete secrets hf-secret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2acSuqPeNjJJ"
      },
      "outputs": [],
      "source": [
        "! gcloud container clusters delete {CLUSTER_NAME} \\\n",
        "  --region={REGION} \\\n",
        "  --quiet"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "serve-gemma-on-gke-using-vllm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
